{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffTgb9f-dvS1",
        "outputId": "5035c1bc-057d-4a60-b35c-aac41af8ac4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'labels', 'id'],\n",
              "        num_rows: 43410\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'labels', 'id'],\n",
              "        num_rows: 5426\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'labels', 'id'],\n",
              "        num_rows: 5427\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"google-research-datasets/go_emotions\")\n",
        "\n",
        "# Access splits\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n",
        "test_data = dataset['test']\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRJX_FryeG3F",
        "outputId": "4d332561-78f5-4d36-f485-8320f8632a22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \"My favourite food is anything I didn't have to cook myself.\",\n",
              " 'labels': [27],\n",
              " 'id': 'eebbqej'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaP3ecUPeTy8",
        "outputId": "6d52bc28-c407-4bac-a7c0-72ac1c16a7f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Is this in New Orleans?? I really feel like this is New Orleans.',\n",
              " 'labels': [27],\n",
              " 'id': 'edgurhb'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZOPxTMdeWb8",
        "outputId": "56470d57-3b85-4c24-b39a-e70bfd36d933"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Iâ€™m really sorry about your situation :( Although I love the names Sapphira, Cirilla, and Scarlett!',\n",
              " 'labels': [25],\n",
              " 'id': 'eecwqtt'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_data['text']\n",
        "y_train = train_data['labels']\n",
        "\n",
        "X_val = val_data['text']\n",
        "y_val = val_data['labels']\n",
        "\n",
        "X_test = test_data['text']\n",
        "y_test = test_data['labels']"
      ],
      "metadata": {
        "id": "OMKeMYlxUSYd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmSLIC-MWpGw",
        "outputId": "41628275-7814-40b0-8505-9ee95e7eb889"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column([\"My favourite food is anything I didn't have to cook myself.\", 'Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead', 'WHY THE FUCK IS BAYLESS ISOING', 'To make her feel threatened', 'Dirty Southern Wankers'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Replicates CountVectorizer's default tokenization + adds lemmatization\n",
        "    \"\"\"\n",
        "    # Step 1: Lowercase (default behavior)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Extract tokens using default regex pattern\n",
        "    # Pattern: (?u)\\b\\w\\w+\\b means \"2 or more word characters\"\n",
        "    token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
        "    tokens = re.findall(token_pattern, text)\n",
        "\n",
        "    # Step 3: Lemmatize each token (THE ONLY DIFFERENCE)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "20VT0-n2miPP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define configurations to test\n",
        "configs = [\n",
        "    # Unigrams only\n",
        "    {'max_features': 5000, 'ngram_range': (1,1), 'name': 'unigram_5k'},\n",
        "    {'max_features': 10000, 'ngram_range': (1,1), 'name': 'unigram_10k'},\n",
        "    {'max_features': None, 'ngram_range': (1,1), 'name': 'unigram_all'},\n",
        "\n",
        "    # Bigrams only\n",
        "    {'max_features': 5000, 'ngram_range': (2,2), 'name': 'bigram_5k'},\n",
        "    {'max_features': 10000, 'ngram_range': (2,2), 'name': 'bigram_10k'},\n",
        "    {'max_features': None, 'ngram_range': (2,2), 'name': 'bigram_all'},\n",
        "\n",
        "    # Unigrams + Bigrams combined\n",
        "    {'max_features': 5000, 'ngram_range': (1,2), 'name': 'uni+bi_5k'},\n",
        "    {'max_features': 10000, 'ngram_range': (1,2), 'name': 'uni+bi_10k'},\n",
        "    {'max_features': 15000, 'ngram_range': (1,2), 'name': 'uni+bi_15k'},\n",
        "    {'max_features': None, 'ngram_range': (1,2), 'name': 'uni+bi_all'},\n",
        "\n",
        "    {'max_features': 5000, 'ngram_range': (1,2), 'tokenizer': lemmatize_tokenizer, 'name': 'uni+bi_5k_lemma'},\n",
        "]\n",
        "\n",
        "# Create vectorizers\n",
        "vectorizers = []\n",
        "for config in configs:\n",
        "    name = config.pop('name')  # Remove name from config\n",
        "    vectorizer = CountVectorizer(\n",
        "        min_df=2,\n",
        "        max_df=0.8,\n",
        "        **config  # Unpack remaining parameters\n",
        "    )\n",
        "    vectorizers.append((name, vectorizer))\n",
        "\n",
        "print(f\"Total configurations to test: {len(vectorizers)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdTq30uqS3uZ",
        "outputId": "c0ab6a85-e4e9-4d8e-8062-bbad6938116a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total configurations to test: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Convert multi-label format to binary matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_binary = mlb.fit_transform(y_train)\n",
        "y_val_binary = mlb.transform(y_val)\n",
        "y_test_binary = mlb.transform(y_test)\n",
        "\n",
        "print(f\"Number of emotion classes: {len(mlb.classes_)}\")\n",
        "print(f\"y_train shape: {y_train_binary.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHoPvYy5ZyYy",
        "outputId": "fba5ef40-d678-4a7b-c1c9-163158fcbd2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of emotion classes: 28\n",
            "y_train shape: (43410, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, hamming_loss\n",
        "import time\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, vectorizer in vectorizers:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training with: {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Transform data\n",
        "    start_time = time.time()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_val_vec = vectorizer.transform(X_val)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    transform_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Feature matrix shape: {X_train_vec.shape}\")\n",
        "    print(f\"Actual vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "    print(f\"Transform time: {transform_time:.2f}s\")\n",
        "\n",
        "    # Train classifier\n",
        "    print(\"Training classifier...\")\n",
        "    start_time = time.time()\n",
        "    classifier = OneVsRestClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    )\n",
        "    classifier.fit(X_train_vec, y_train_binary)\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"Training time: {train_time:.2f}s\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_val_pred = classifier.predict(X_val_vec)\n",
        "\n",
        "    val_accuracy = accuracy_score(y_val_binary, y_val_pred)\n",
        "    val_f1_micro = f1_score(y_val_binary, y_val_pred, average='micro')\n",
        "    val_f1_macro = f1_score(y_val_binary, y_val_pred, average='macro')\n",
        "    val_hamming = hamming_loss(y_val_binary, y_val_pred)\n",
        "\n",
        "    print(f\"\\nValidation Results:\")\n",
        "    print(f\"  Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"  F1 (micro): {val_f1_micro:.4f}\")\n",
        "    print(f\"  F1 (macro): {val_f1_macro:.4f}\")\n",
        "    print(f\"  Hamming Loss: {val_hamming:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        'name': name,\n",
        "        'vocab_size': len(vectorizer.vocabulary_),\n",
        "        'accuracy': val_accuracy,\n",
        "        'f1_micro': val_f1_micro,\n",
        "        'f1_macro': val_f1_macro,\n",
        "        'hamming_loss': val_hamming,\n",
        "        'train_time': train_time,\n",
        "        'transform_time': transform_time\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Y-QyEMZ1ok",
        "outputId": "1f7178de-7506-4876-ab63-89eb9f17121a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training with: unigram_5k\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 5000)\n",
            "Actual vocabulary size: 5000\n",
            "Transform time: 8.22s\n",
            "Training classifier...\n",
            "Training time: 16.44s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3496\n",
            "  F1 (micro): 0.4881\n",
            "  F1 (macro): 0.3449\n",
            "  Hamming Loss: 0.0343\n",
            "\n",
            "============================================================\n",
            "Training with: unigram_10k\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 10000)\n",
            "Actual vocabulary size: 10000\n",
            "Transform time: 2.08s\n",
            "Training classifier...\n",
            "Training time: 13.45s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3481\n",
            "  F1 (micro): 0.4877\n",
            "  F1 (macro): 0.3421\n",
            "  Hamming Loss: 0.0343\n",
            "\n",
            "============================================================\n",
            "Training with: unigram_all\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 13077)\n",
            "Actual vocabulary size: 13077\n",
            "Transform time: 2.04s\n",
            "Training classifier...\n",
            "Training time: 29.23s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3491\n",
            "  F1 (micro): 0.4881\n",
            "  F1 (macro): 0.3423\n",
            "  Hamming Loss: 0.0342\n",
            "\n",
            "============================================================\n",
            "Training with: bigram_5k\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 5000)\n",
            "Actual vocabulary size: 5000\n",
            "Transform time: 4.31s\n",
            "Training classifier...\n",
            "Training time: 5.01s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.1635\n",
            "  F1 (micro): 0.2812\n",
            "  F1 (macro): 0.1735\n",
            "  Hamming Loss: 0.0396\n",
            "\n",
            "============================================================\n",
            "Training with: bigram_10k\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 10000)\n",
            "Actual vocabulary size: 10000\n",
            "Transform time: 3.31s\n",
            "Training classifier...\n",
            "Training time: 8.76s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.1753\n",
            "  F1 (micro): 0.2960\n",
            "  F1 (macro): 0.1804\n",
            "  Hamming Loss: 0.0395\n",
            "\n",
            "============================================================\n",
            "Training with: bigram_all\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 45261)\n",
            "Actual vocabulary size: 45261\n",
            "Transform time: 2.86s\n",
            "Training classifier...\n",
            "Training time: 30.92s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.1784\n",
            "  F1 (micro): 0.2974\n",
            "  F1 (macro): 0.1701\n",
            "  Hamming Loss: 0.0392\n",
            "\n",
            "============================================================\n",
            "Training with: uni+bi_5k\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 5000)\n",
            "Actual vocabulary size: 5000\n",
            "Transform time: 4.85s\n",
            "Training classifier...\n",
            "Training time: 11.34s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3542\n",
            "  F1 (micro): 0.4953\n",
            "  F1 (macro): 0.3572\n",
            "  Hamming Loss: 0.0344\n",
            "\n",
            "============================================================\n",
            "Training with: uni+bi_10k\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 10000)\n",
            "Actual vocabulary size: 10000\n",
            "Transform time: 4.01s\n",
            "Training classifier...\n",
            "Training time: 14.19s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3485\n",
            "  F1 (micro): 0.4880\n",
            "  F1 (macro): 0.3534\n",
            "  Hamming Loss: 0.0348\n",
            "\n",
            "============================================================\n",
            "Training with: uni+bi_15k\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 15000)\n",
            "Actual vocabulary size: 15000\n",
            "Transform time: 3.87s\n",
            "Training classifier...\n",
            "Training time: 37.78s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3454\n",
            "  F1 (micro): 0.4854\n",
            "  F1 (macro): 0.3474\n",
            "  Hamming Loss: 0.0348\n",
            "\n",
            "============================================================\n",
            "Training with: uni+bi_all\n",
            "============================================================\n",
            "Feature matrix shape: (43410, 58338)\n",
            "Actual vocabulary size: 58338\n",
            "Transform time: 3.33s\n",
            "Training classifier...\n",
            "Training time: 51.73s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3428\n",
            "  F1 (micro): 0.4861\n",
            "  F1 (macro): 0.3414\n",
            "  Hamming Loss: 0.0343\n",
            "\n",
            "============================================================\n",
            "Training with: uni+bi_5k_lemma\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape: (43410, 5000)\n",
            "Actual vocabulary size: 5000\n",
            "Transform time: 12.50s\n",
            "Training classifier...\n",
            "Training time: 11.89s\n",
            "\n",
            "Validation Results:\n",
            "  Accuracy: 0.3559\n",
            "  F1 (micro): 0.4950\n",
            "  F1 (macro): 0.3609\n",
            "  Hamming Loss: 0.0344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create comparison table\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('f1_micro', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTS COMPARISON (sorted by F1-micro)\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frs-RtCZaI4G",
        "outputId": "f5405faa-4e3a-487a-e678-93aaa5699fbe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RESULTS COMPARISON (sorted by F1-micro)\n",
            "================================================================================\n",
            "           name  vocab_size  accuracy  f1_micro  f1_macro  hamming_loss  train_time  transform_time\n",
            "      uni+bi_5k        5000  0.354220  0.495269  0.357238      0.034411   11.335574        4.851443\n",
            "uni+bi_5k_lemma        5000  0.355879  0.494979  0.360937      0.034424   11.890828       12.497140\n",
            "    unigram_all       13077  0.349060  0.488118  0.342288      0.034168   29.234254        2.044244\n",
            "     unigram_5k        5000  0.349613  0.488116  0.344946      0.034306   16.440951        8.222292\n",
            "     uni+bi_10k       10000  0.348507  0.488008  0.353413      0.034845   14.192036        4.006302\n",
            "    unigram_10k       10000  0.348139  0.487651  0.342120      0.034273   13.453387        2.084634\n",
            "     uni+bi_all       58338  0.342794  0.486108  0.341363      0.034332   51.732908        3.332034\n",
            "     uni+bi_15k       15000  0.345374  0.485414  0.347416      0.034832   37.777761        3.865401\n",
            "     bigram_all       45261  0.178400  0.297406  0.170118      0.039216   30.918966        2.855746\n",
            "     bigram_10k       10000  0.175267  0.295983  0.180368      0.039453    8.758461        3.314913\n",
            "      bigram_5k        5000  0.163472  0.281239  0.173461      0.039565    5.007204        4.306410\n"
          ]
        }
      ]
    }
  ]
}